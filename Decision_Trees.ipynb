{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate Question Pair Detection in Quora Question Pairs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/sarthak/Desktop/SMAI_Project/Decision_Trees.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sarthak/Desktop/SMAI_Project/Decision_Trees.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sarthak/Desktop/SMAI_Project/Decision_Trees.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sarthak/Desktop/SMAI_Project/Decision_Trees.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sarthak/Desktop/SMAI_Project/Decision_Trees.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sarthak/Desktop/SMAI_Project/Decision_Trees.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# importing required libraries for duplicquestion detection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pun(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    return text\n",
    "\n",
    "def clean_dataset(df):\n",
    "    #drop na\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep=~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    df=df[indices_to_keep].astype(np.float64)\n",
    "    return df\n",
    "\n",
    "def length(text):\n",
    "    return len(text)\n",
    "\n",
    "def diff_length(text1, text2):\n",
    "    return abs(len(text1) - len(text2))\n",
    "\n",
    "def len_ratio(text1,text2):\n",
    "    return float(len(text1)/len(text2))\n",
    "\n",
    "def com_low(text):\n",
    "    text1=text[0]\n",
    "    text2=text[1]\n",
    "    #split text1 and 2\n",
    "    text1=text1.split()\n",
    "    text2=text2.split()\n",
    "    #check for lower case\n",
    "    test1_low=set()\n",
    "    test2_low=set()\n",
    "    # check if the word is lower case or not\n",
    "    for word in text1:\n",
    "        if word.islower():\n",
    "            test1_low.add(word)\n",
    "    for word in text2:\n",
    "        if word.islower():\n",
    "            test2_low.add(word)\n",
    "    #check for common lower case words\n",
    "    common_low=test1_low.intersection(test2_low)\n",
    "    return len(list(common_low))\n",
    "\n",
    "def stop(text):\n",
    "    text1=text[0]\n",
    "    text2=text[1]\n",
    "    #split text1 and 2\n",
    "    text1=text1.split()\n",
    "    text2=text2.split()\n",
    "    #check for stop words\n",
    "    test1_stop=set()\n",
    "    test2_stop=set()\n",
    "    # check if the word is stop word or not\n",
    "    for word in text1:\n",
    "        if word in stopwords.words('english'):\n",
    "            test1_stop.add(word)\n",
    "    for word in text2:\n",
    "        if word in stopwords.words('english'):\n",
    "            test2_stop.add(word)\n",
    "    #check for common stop words\n",
    "    common_stop=test1_stop.intersection(test2_stop)\n",
    "    return len(list(common_stop))\n",
    "\n",
    "def last(text):\n",
    "    text1=text[0]\n",
    "    text2=text[1]\n",
    "    #split text1 and 2\n",
    "    text1=text1.split()\n",
    "    text2=text2.split()\n",
    "    #check for last word\n",
    "    if text1[-1]==text2[-1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def com_cap(text):\n",
    "    text1=text[0]\n",
    "    text2=text[1]\n",
    "    #split text1 and 2\n",
    "    text1=text1.split()\n",
    "    text2=text2.split()\n",
    "    #check for common capital words\n",
    "    test1_cap=set()\n",
    "    test2_cap=set()\n",
    "    # check if the word is capital or not\n",
    "    for word in text1:\n",
    "        if word.isupper():\n",
    "            test1_cap.add(word)\n",
    "    for word in text2:\n",
    "        if word.isupper():\n",
    "            test2_cap.add(word)\n",
    "    #check for common capital words\n",
    "    common_cap=test1_cap.intersection(test2_cap)\n",
    "    return len(list(common_cap))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('3.9.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f41c49a37b4cb3062a0a8a444e02de43de218a1a5aeb2bbc8c7b02ec73503f26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
