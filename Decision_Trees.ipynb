{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate Question Pair Detection in Quora Question Pairs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/sarthak/.pyenv/versions/3.9.13/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/sarthak/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: click in /Users/sarthak/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/sarthak/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in /Users/sarthak/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/Users/sarthak/.pyenv/versions/3.9.13/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sarthak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries for duplicquestion detection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pun(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    return text\n",
    "\n",
    "def clean_dataset(df):\n",
    "    #drop na\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep=~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    df=df[indices_to_keep].astype(np.float64)\n",
    "    return df\n",
    "\n",
    "def length(text):\n",
    "    return len(text)\n",
    "\n",
    "def diff_length(text1, text2):\n",
    "    return abs(len(text1) - len(text2))\n",
    "\n",
    "def len_ratio(text1,text2):\n",
    "    return float(len(text1)/len(text2))\n",
    "\n",
    "def com_low(text):\n",
    "    text1=text[0]\n",
    "    text2=text[1]\n",
    "    #split text1 and 2\n",
    "    text1=text1.split()\n",
    "    text2=text2.split()\n",
    "    #check for lower case\n",
    "    test1_low=set()\n",
    "    test2_low=set()\n",
    "    # check if the word is lower case or not\n",
    "    for word in text1:\n",
    "        if word.islower():\n",
    "            test1_low.add(word)\n",
    "    for word in text2:\n",
    "        if word.islower():\n",
    "            test2_low.add(word)\n",
    "    #check for common lower case words\n",
    "    common_low=test1_low.intersection(test2_low)\n",
    "    return len(list(common_low))\n",
    "\n",
    "def stop(text):\n",
    "    text1=text[0]\n",
    "    text2=text[1]\n",
    "    #split text1 and 2\n",
    "    text1=text1.split()\n",
    "    text2=text2.split()\n",
    "    #check for stop words\n",
    "    text1_low=set()\n",
    "    text2_low=set()\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    # check if the word is stop word or not\n",
    "    for word in text1:\n",
    "        #checkif word is lower case\n",
    "        if word.islower():\n",
    "            text1_low.add(word)\n",
    "    \n",
    "    text1=list(text1_low)\n",
    "    for word in text2:\n",
    "        #check if word is lower case\n",
    "        if word.islower():\n",
    "            text2_low.add(word)\n",
    "    text2=list(text2_low)\n",
    "    #check for lowercase as well as stop_words\n",
    "    text1=[w for w in text1 if not w.lower() in stop_words]\n",
    "    text2=[w for w in text2 if not w.lower() in stop_words]\n",
    "    text1=set(text1)\n",
    "    text2=set(text2)\n",
    "    #check for common stop words\n",
    "    common_stop=text1.intersection(text2)\n",
    "    return len(list(common_stop))\n",
    "\n",
    "def last(text):\n",
    "    text1=text[0]\n",
    "    text2=text[1]\n",
    "    #split text1 and 2\n",
    "    text1=list(text1.split(\" \"))\n",
    "    text2=list(text2.split(\" \"))\n",
    "    #check for last word\n",
    "    if text1[len(list(text1))-1]==text2[len(list(text2))-1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def com_cap(text):\n",
    "    text1=text[0]\n",
    "    text2=text[1]\n",
    "    #split text1 and 2\n",
    "    text1=text1.split()\n",
    "    text2=text2.split()\n",
    "    #check for common capital words\n",
    "    test1_cap=set()\n",
    "    test2_cap=set()\n",
    "    # check if the word is capital or not\n",
    "    for word in text1:\n",
    "        if word.isupper():\n",
    "            test1_cap.add(word)\n",
    "    for word in text2:\n",
    "        if word.isupper():\n",
    "            test2_cap.add(word)\n",
    "    #check for common capital words\n",
    "    common_cap=test1_cap.intersection(test2_cap)\n",
    "    return len(list(common_cap))\n",
    "\n",
    "def pre(text,n):\n",
    "    text1=text[0]\n",
    "    text2=text[1]\n",
    "    #split text1 and 2\n",
    "    text1=text1.split()\n",
    "    text2=text2.split()\n",
    "    #check for common prefix\n",
    "    str1=text1\n",
    "    str2=text2\n",
    "    str1.sort()\n",
    "    i=0\n",
    "    j=0\n",
    "    str2.sort()\n",
    "    ans = 0\n",
    "    while True:\n",
    "        if i >= len(str1) or j >= len(str2):\n",
    "            break\n",
    "        if len(str1[i]) < n:\n",
    "            i += 1\n",
    "            continue\n",
    "        if len(str2[j]) < n:\n",
    "            j += 1\n",
    "            continue\n",
    "        s1 = str1[i][0:n]\n",
    "        s2 = str2[j][0:n]\n",
    "        if s1 == s2:\n",
    "            ans += 1\n",
    "            i += 1 \n",
    "            j += 1 \n",
    "        else:\n",
    "            if str1[i]>str2[j]:\n",
    "                j += 1\n",
    "            else:\n",
    "                i += 1\n",
    "    return ans\n",
    "\n",
    "def misc1(text):\n",
    "    #check if not is present of not\n",
    "    text=text.split()\n",
    "    if 'not' in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def misc2(text):\n",
    "    text1=text[0]\n",
    "    text2=text[1]\n",
    "    #split text1 and 2\n",
    "    text1=text1.split()\n",
    "    text2=text2.split()\n",
    "    #check is not is present in both\n",
    "    if 'not' in text1 and 'not' in text2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def misc3(text):\n",
    "    text1=text[0]\n",
    "    text2=text[1]\n",
    "    #split text1 and 2\n",
    "    text1=text1.split()\n",
    "    text2=text2.split()\n",
    "    #check if word is in digit and is in text2 as well\n",
    "    for word in text1:\n",
    "        if word.isdigit():\n",
    "            if word in text2:\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "def misc4(text):\n",
    "    text1=text[0]\n",
    "    text2=text[1]\n",
    "    ps=PorterStemmer()\n",
    "    #tokenize word1 and word2\n",
    "    text1=text1.split()\n",
    "    text2=text2.split()\n",
    "    text_s1=[]\n",
    "    text_s2=[]\n",
    "    #stemming\n",
    "    for word in text1:\n",
    "        text_s1.append(ps.stem(word))\n",
    "    for word in text2:\n",
    "        text_s2.append(ps.stem(word))\n",
    "    text1_low=set()\n",
    "    text2_low=set()\n",
    "    # check if the word is lower case or not\n",
    "    for word in text_s1:\n",
    "        if word.islower():\n",
    "            text1_low.add(word)\n",
    "    for word in text_s2:\n",
    "        if word.islower():\n",
    "            text2_low.add(word)\n",
    "    #check for common lower case words\n",
    "    common_low=text1_low.intersection(text2_low)\n",
    "    return len(list(common_low))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./train.csv\")\n",
    "data = data.dropna(how=\"any\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply pun function\n",
    "data['question1']=data['question1'].apply(pun)\n",
    "data['question2']=data['question2'].apply(pun)\n",
    "#apply lenght function\n",
    "data['l1']=data['question1'].apply(length)\n",
    "data['l2']=data['question2'].apply(length)\n",
    "#apply diff_length function\n",
    "data['l3']=abs(data['l1']-data['l2'])\n",
    "#apply len_ratio function\n",
    "data['l4']=(data['l1']/data['l2'])\n",
    "#copy data in to data_l\n",
    "data_l=data.copy()\n",
    "#apply com_low function\n",
    "data['lc1']=data[['question1','question2']].apply(com_low,axis=1)\n",
    "\n",
    "sen1=data['question1'].str.len().idxmax()\n",
    "sen2=data['question2'].str.len().idxmax()\n",
    "#get the longest sentence\n",
    "l1=len(data['question1'][sen1])\n",
    "l2=len(data['question2'][sen2])\n",
    "#get max between l1 and l2\n",
    "l=max(l1,l2)\n",
    "data['lc2']=data[['question1','question2']].apply(com_low,axis=1)\n",
    "data['lc2']=data['lc2']/l\n",
    "#copy data in to data_2\n",
    "data_2=data.copy()\n",
    "data_2.head()\n",
    "#data['lcxs_1']=data[['question1','question2']].apply(lambda x:stop(x),axis=1)\n",
    "#data['lxcs_2']=(data['lcxs_1'])/l\n",
    "#copy data in to data_3\n",
    "data_3=data.copy()\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lw_1']=data[['question1','question2']].apply(last,axis=1)\n",
    "data_4=data.copy()\n",
    "data['cap_1']=data[['question1','question2']].apply(com_cap,axis=1)\n",
    "data['cap_2']=data['cap_1']/l\n",
    "data_5=data.copy()\n",
    "\n",
    "#apply all the prefix function\n",
    "data['pre_1']=data[['question1','question2']].apply(lambda x:pre(x,3),axis=1)\n",
    "data['pre_3']=data[['question1','question2']].apply(lambda x:pre(x,4),axis=1)\n",
    "data['pre_5']=data[['question1','question2']].apply(lambda x:pre(x,5),axis=1)\n",
    "data['pre_7']=data[['question1','question2']].apply(lambda x:pre(x,6),axis=1)\n",
    "data['pre_2']=data['pre_1']/l\n",
    "data['pre_4']=data['pre_3']/l\n",
    "data['pre_6']=data['pre_5']/l\n",
    "data['pre_8']=data['pre_7']/l\n",
    "data_6=data.copy()\n",
    "\n",
    "#apply all the misc function\n",
    "data['misc1'] = data['question1'].apply(misc1)\n",
    "data['misc2'] = data['question2'].apply(misc1)\n",
    "\n",
    "data['misc3'] = data[['question1','question2']].apply(misc2,axis=1)\n",
    "data['misc4'] = data[['question1','question2']].apply(misc3,axis=1)\n",
    "data['misc5']= data[['question1','question2']].apply(lambda x:misc4(x),axis=1)\n",
    "data_7=data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data splitting and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>l3</th>\n",
       "      <th>l4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>56</td>\n",
       "      <td>9</td>\n",
       "      <td>1.160714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor KohiNoor Diamond</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>83</td>\n",
       "      <td>37</td>\n",
       "      <td>0.554217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>58</td>\n",
       "      <td>14</td>\n",
       "      <td>1.241379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely How can I solve it</td>\n",
       "      <td>Find the remainder when math2324math is divide...</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>0.872727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar salt ...</td>\n",
       "      <td>Which fish would survive in salt water</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>38</td>\n",
       "      <td>35</td>\n",
       "      <td>1.921053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4     What is the story of Kohinoor KohiNoor Diamond   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8   Why am I mentally very lonely How can I solve it   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar salt ...   \n",
       "\n",
       "                                           question2  is_duplicate  l1  l2  \\\n",
       "0  What is the step by step guide to invest in sh...             0  65  56   \n",
       "1  What would happen if the Indian government sto...             0  46  83   \n",
       "2  How can Internet speed be increased by hacking...             0  72  58   \n",
       "3  Find the remainder when math2324math is divide...             0  48  55   \n",
       "4             Which fish would survive in salt water             0  73  38   \n",
       "\n",
       "   l3        l4  \n",
       "0   9  1.160714  \n",
       "1  37  0.554217  \n",
       "2  14  1.241379  \n",
       "3   7  0.872727  \n",
       "4  35  1.921053  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_l = data_l.dropna(axis=0, how='any', subset=None, inplace=False)\n",
    "data_l.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y4/yphzsd_j3hbd5rzqwpdq7x9m0000gn/T/ipykernel_28679/593163608.py:8: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n",
      "  indices_to_keep=~df.isin([np.nan, np.inf, -np.inf]).any(1)\n"
     ]
    }
   ],
   "source": [
    "data_l.drop(['id', 'qid1', 'qid2', 'question1', 'question2'], inplace=True,axis=1)\n",
    "data_l.head()\n",
    "clean_dataset(data_l)\n",
    "\n",
    "y_l=data_l['is_duplicate']\n",
    "data_l.drop(['is_duplicate'], inplace=True,axis=1)\n",
    "x_train_l, x_test_l, y_train_l, y_test_l = train_test_split(data_l, y_l, test_size=0.2, random_state=42)\n",
    "x_val_l, x_test_l, y_val_l, y_test_l = train_test_split(x_test_l, y_test_l, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6378342279057112\n",
      "F1 score: 0.3128402477942557\n"
     ]
    }
   ],
   "source": [
    "dt=DecisionTreeClassifier(max_depth=10,min_samples_leaf=10)\n",
    "x_train_l = x_train_l.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "y_train_l = y_train_l.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "x_val_l = x_val_l.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "dt.fit(x_train_l,y_train_l)\n",
    "y_pred_l=dt.predict(x_test_l)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_l, y_pred_l))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_l, y_pred_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.638749412550397\n",
      "F1 score: 0.3140938336542526\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier(n_estimators=100,max_depth=30,min_samples_leaf=100)\n",
    "rf.fit(x_train_l,y_train_l)\n",
    "y_pred_l=rf.predict(x_test_l)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_l, y_pred_l))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_l, y_pred_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6377352890252047\n",
      "F1 score: 0.32344789356984477\n"
     ]
    }
   ],
   "source": [
    "gb=GradientBoostingClassifier(n_estimators=500,max_depth=10,min_samples_leaf=100)\n",
    "gb.fit(x_train_l,y_train_l)\n",
    "y_pred_l=gb.predict(x_test_l)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_l, y_pred_l))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_l, y_pred_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Lowercased Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y4/yphzsd_j3hbd5rzqwpdq7x9m0000gn/T/ipykernel_28679/593163608.py:8: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n",
      "  indices_to_keep=~df.isin([np.nan, np.inf, -np.inf]).any(1)\n"
     ]
    }
   ],
   "source": [
    "# data_2.head()\n",
    "data_2 = data_2.dropna(axis=0, how='any', subset=None, inplace=False)\n",
    "data_2.drop(['id', 'qid1', 'qid2', 'question1', 'question2'], inplace=True,axis=1)\n",
    "clean_dataset(data_2)\n",
    "y_2 = data_2['is_duplicate']\n",
    "data_2.drop('is_duplicate', inplace=True,axis=1)\n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(data_2, y_2,  test_size=0.30, random_state=25)\n",
    "x_val_2, x_test_2, y_val_2, y_test_2 = train_test_split(x_test_2, y_test_2 ,test_size=float(1/3), random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6696430779885726\n",
      "F1 score: 0.5532811559301626\n"
     ]
    }
   ],
   "source": [
    "dt=DecisionTreeClassifier(max_depth=10,min_samples_leaf=10)\n",
    "x_train_2 = x_train_2.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "y_train_2 = y_train_2.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "x_val_2 = x_val_2.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)  \n",
    "dt.fit(x_train_2,y_train_2)\n",
    "y_pred_2=dt.predict(x_test_2)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_2, y_pred_2))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_2, y_pred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6736501026490885\n",
      "F1 score: 0.5460675703571183\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier(n_estimators=100,max_depth=30,min_samples_leaf=100)\n",
    "rf.fit(x_train_2,y_train_2)\n",
    "y_pred_2=rf.predict(x_test_2)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_2, y_pred_2))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_2, y_pred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common lowercased words excluding stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y4/yphzsd_j3hbd5rzqwpdq7x9m0000gn/T/ipykernel_28679/593163608.py:8: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n",
      "  indices_to_keep=~df.isin([np.nan, np.inf, -np.inf]).any(1)\n"
     ]
    }
   ],
   "source": [
    "data3 = data_3.dropna(axis=0, how='any', subset=None, inplace=False)\n",
    "data3.drop(['id', 'qid1', 'qid2', 'question1', 'question2'], inplace=True,axis=1)\n",
    "clean_dataset(data3)\n",
    "y_3 = data3['is_duplicate']\n",
    "data3.drop('is_duplicate', inplace=True,axis=1)\n",
    "x_train_3, x_test_3, y_train_3, y_test_3 = train_test_split(data3, y_3,  test_size=0.30, random_state=25)\n",
    "x_val_3, x_test_3, y_val_3, y_test_3 = train_test_split(x_test_3, y_test_3 ,test_size=float(1/3), random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6695688738281926\n",
      "F1 score: 0.553076176775618\n"
     ]
    }
   ],
   "source": [
    "dt=DecisionTreeClassifier(max_depth=10,min_samples_leaf=10)\n",
    "x_train_3 = x_train_3.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "y_train_3 = y_train_3.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "x_val_3 = x_val_3.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "dt.fit(x_train_3,y_train_3)\n",
    "y_pred_3=dt.predict(x_test_3)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_3, y_pred_3))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_3, y_pred_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6730812040861758\n",
      "F1 score: 0.5454170249355116\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier(n_estimators=100,max_depth=30,min_samples_leaf=100)\n",
    "rf.fit(x_train_3,y_train_3)\n",
    "y_pred_3=rf.predict(x_test_3)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_3, y_pred_3))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_3, y_pred_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6697667515892057\n",
      "F1 score: 0.5468246155934964\n"
     ]
    }
   ],
   "source": [
    "gb=GradientBoostingClassifier(n_estimators=500,max_depth=10,min_samples_leaf=100)\n",
    "gb.fit(x_train_3,y_train_3)\n",
    "y_pred_3=gb.predict(x_test_3)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_3, y_pred_3))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_3, y_pred_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same last word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y4/yphzsd_j3hbd5rzqwpdq7x9m0000gn/T/ipykernel_28679/593163608.py:8: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n",
      "  indices_to_keep=~df.isin([np.nan, np.inf, -np.inf]).any(1)\n"
     ]
    }
   ],
   "source": [
    "data4 = data_4.dropna(axis=0, how='any', subset=None, inplace=False)\n",
    "data4.drop(['id', 'qid1', 'qid2', 'question1', 'question2'], inplace=True,axis=1)\n",
    "clean_dataset(data4)\n",
    "y_4 = data4['is_duplicate']\n",
    "data4.drop('is_duplicate', inplace=True,axis=1)\n",
    "x_train_4, x_test_4, y_train_4, y_test_4 = train_test_split(data4, y_4,  test_size=0.30, random_state=25)\n",
    "x_val_4, x_test_4, y_val_4, y_test_4 = train_test_split(x_test_4, y_test_4 ,test_size=float(1/3), random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7033317668010587\n",
      "F1 score: 0.5318866599016471\n"
     ]
    }
   ],
   "source": [
    "dt=DecisionTreeClassifier(max_depth=10,min_samples_leaf=10)\n",
    "x_train_4 = x_train_4.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "y_train_4 = y_train_4.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "x_val_4 = x_val_4.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "dt.fit(x_train_4,y_train_4)\n",
    "y_pred_4=dt.predict(x_test_4)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_4, y_pred_4))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_4, y_pred_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7051126666501768\n",
      "F1 score: 0.5590650196020416\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier(n_estimators=100,max_depth=30,min_samples_leaf=100)\n",
    "rf.fit(x_train_4,y_train_4)\n",
    "y_pred_4=rf.predict(x_test_4)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_4, y_pred_4))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_4, y_pred_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7023671127161196\n",
      "F1 score: 0.5641322852899627\n"
     ]
    }
   ],
   "source": [
    "gb=GradientBoostingClassifier(n_estimators=500,max_depth=10,min_samples_leaf=100)\n",
    "gb.fit(x_train_4,y_train_4)\n",
    "y_pred_4=gb.predict(x_test_4)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_4, y_pred_4))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_4, y_pred_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of common Capitalized Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y4/yphzsd_j3hbd5rzqwpdq7x9m0000gn/T/ipykernel_28679/593163608.py:8: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n",
      "  indices_to_keep=~df.isin([np.nan, np.inf, -np.inf]).any(1)\n"
     ]
    }
   ],
   "source": [
    "data5 = data_5.dropna(axis=0, how='any', subset=None, inplace=False)\n",
    "data5.drop(['id', 'qid1', 'qid2', 'question1', 'question2'], inplace=True,axis=1)\n",
    "clean_dataset(data5)\n",
    "y_5 = data5['is_duplicate']\n",
    "data5.drop('is_duplicate', inplace=True,axis=1)\n",
    "x_train_5, x_test_5, y_train_5, y_test_5 = train_test_split(data5, y_5,  test_size=0.30, random_state=25)\n",
    "x_val_5, x_test_5, y_val_5, y_test_5 = train_test_split(x_test_5, y_test_5 ,test_size=float(1/3), random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7038017264834648\n",
      "F1 score: 0.5364097402346019\n"
     ]
    }
   ],
   "source": [
    "dt=DecisionTreeClassifier(max_depth=10,min_samples_leaf=10)\n",
    "x_train_5 = x_train_5.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "y_train_5 = y_train_5.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "x_val_5 = x_val_5.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "dt.fit(x_train_5,y_train_5)\n",
    "y_pred_5=dt.predict(x_test_5)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_5, y_pred_5))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_5, y_pred_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7071161789804349\n",
      "F1 score: 0.5669775095995612\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier(n_estimators=100,max_depth=30,min_samples_leaf=100)\n",
    "rf.fit(x_train_5,y_train_5)\n",
    "y_pred_5=rf.predict(x_test_5)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_5, y_pred_5))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_5, y_pred_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7061762596156225\n",
      "F1 score: 0.5747019440764741\n"
     ]
    }
   ],
   "source": [
    "gb=GradientBoostingClassifier(n_estimators=500,max_depth=10,min_samples_leaf=100)\n",
    "gb.fit(x_train_5,y_train_5)\n",
    "y_pred_5=gb.predict(x_test_5)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_5, y_pred_5))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_5, y_pred_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commmon prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y4/yphzsd_j3hbd5rzqwpdq7x9m0000gn/T/ipykernel_28679/593163608.py:8: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n",
      "  indices_to_keep=~df.isin([np.nan, np.inf, -np.inf]).any(1)\n"
     ]
    }
   ],
   "source": [
    "data6 = data_6.dropna(axis=0, how='any', subset=None, inplace=False)\n",
    "data6.drop(['id', 'qid1', 'qid2', 'question1', 'question2'], inplace=True,axis=1)\n",
    "clean_dataset(data6)\n",
    "y_6 = data6['is_duplicate']\n",
    "data6.drop('is_duplicate', inplace=True,axis=1)\n",
    "x_train_6, x_test_6, y_train_6, y_test_6 = train_test_split(data6, y_6,  test_size=0.30, random_state=25)\n",
    "x_val_6, x_test_6, y_val_6, y_test_6 = train_test_split(x_test_6, y_test_6 ,test_size=float(1/3), random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7181725988770437\n",
      "F1 score: 0.6193371642389415\n"
     ]
    }
   ],
   "source": [
    "dt=DecisionTreeClassifier(max_depth=10,min_samples_leaf=10)\n",
    "x_train_6 = x_train_6.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "y_train_6 = y_train_6.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "x_val_6 = x_val_6.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "dt.fit(x_train_6,y_train_6)\n",
    "y_pred_6=dt.predict(x_test_6)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_6, y_pred_6))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_6, y_pred_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7306636325409978\n",
      "F1 score: 0.6213707013456659\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier(n_estimators=100,max_depth=30,min_samples_leaf=100)\n",
    "rf.fit(x_train_6,y_train_6)\n",
    "y_pred_6=rf.predict(x_test_6)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_6, y_pred_6))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_6, y_pred_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7341759627989809\n",
      "F1 score: 0.6334958905978243\n"
     ]
    }
   ],
   "source": [
    "gb=GradientBoostingClassifier(n_estimators=500,max_depth=10,min_samples_leaf=100)\n",
    "gb.fit(x_train_6,y_train_6)\n",
    "y_pred_6=gb.predict(x_test_6)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_6, y_pred_6))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_6, y_pred_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y4/yphzsd_j3hbd5rzqwpdq7x9m0000gn/T/ipykernel_28679/593163608.py:8: FutureWarning: In a future version of pandas all arguments of DataFrame.any and Series.any will be keyword-only.\n",
      "  indices_to_keep=~df.isin([np.nan, np.inf, -np.inf]).any(1)\n"
     ]
    }
   ],
   "source": [
    "data7 = data_7.dropna(axis=0, how='any', subset=None, inplace=False)\n",
    "data7.drop(['id', 'qid1', 'qid2', 'question1', 'question2'], inplace=True,axis=1)\n",
    "clean_dataset(data7)\n",
    "y_7 = data7['is_duplicate']\n",
    "data7.drop('is_duplicate', inplace=True,axis=1)\n",
    "x_train_7, x_test_7, y_train_7, y_test_7 = train_test_split(data7, y_7,  test_size=0.30, random_state=25)\n",
    "x_val_7, x_test_7, y_val_7, y_test_7 = train_test_split(x_test_7, y_test_7 ,test_size=float(1/3), random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7242078705879443\n",
      "F1 score: 0.6124435175530065\n"
     ]
    }
   ],
   "source": [
    "dt=DecisionTreeClassifier(max_depth=10,min_samples_leaf=10)\n",
    "x_train_7 = x_train_7.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "y_train_7 = y_train_7.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "x_val_7 = x_val_7.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)\n",
    "dt.fit(x_train_7,y_train_7)\n",
    "y_pred_7=dt.predict(x_test_7)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_7, y_pred_7))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_7, y_pred_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7340028197580944\n",
      "F1 score: 0.6217376011255715\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier(n_estimators=100,max_depth=30,min_samples_leaf=100)\n",
    "rf.fit(x_train_7,y_train_7)\n",
    "y_pred_7=rf.predict(x_test_7)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_7, y_pred_7))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_7, y_pred_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7450592396547033\n",
      "F1 score: 0.6525066585752334\n"
     ]
    }
   ],
   "source": [
    "gb=GradientBoostingClassifier(n_estimators=500,max_depth=10,min_samples_leaf=100)\n",
    "gb.fit(x_train_7,y_train_7)\n",
    "y_pred_7=gb.predict(x_test_7)\n",
    "print(\"Accuracy:\",accuracy_score(y_test_7, y_pred_7))\n",
    "#print f1 score\n",
    "print(\"F1 score:\",f1_score(y_test_7, y_pred_7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('3.9.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f41c49a37b4cb3062a0a8a444e02de43de218a1a5aeb2bbc8c7b02ec73503f26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
